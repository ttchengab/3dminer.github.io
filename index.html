<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="3DMiner"/>
  <meta property="og:description" content="ICCV-2023"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets</h1>
              <h2> <font size="+2.5">ICCV 2023</font></h1>
                <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ttchengab.github.io" target="_blank">Ta-Ying Cheng</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="http://mgadelha.me" target="_blank">Matheus Gadelha</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://storage.googleapis.com/pirk.io/index.html" target="_blank">Soren Pirk</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://imagine.enpc.fr/~groueixt/" target="_blank">Thibault Groueix</a><sup>2</sup>,</span>
                    </br>
                      <span class="author-block">
                        <a href="https://research.adobe.com/person/radomir-mech/" target="_blank">Radomir Mech</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.cs.ox.ac.uk/people/andrew.markham/" target="_blank">Andrew Markham</a><sup>1</sup>,</span>
                          <span class="author-block">
                            <a href="https://en.wikipedia.org/wiki/Niki_Trigoni" target="_blank">Niki Trigoni<sup>1</sup></a>
                  </span>
                  </div>


                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Oxford<br><sup>2</sup> Adobe Research</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Work partially done during internship at Adobe Research</small></span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_3DMiner_Discovering_Shapes_from_Large-Scale_Unannotated_Image_Datasets_ICCV_2023_paper.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://drive.google.com/file/d/1FcD_g9GDevgMGvWRwplGTwyovmhg_djl/view?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ttchengab/3DMiner/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4"> -->

          <img src="static/images/teaser.gif" alt="3DMiner Overview"/>
          <!-- <video controls loop>
            <source src="static/images/teaser_vid.mov">
            Your browser does not support the video tag.
          </video> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        We present 3DMiner, a scalable framework designed to obtain associating poses and reconstruct shapes from
        <em>diverse and realistic</em> sets of images <em>without any 3D data, pose annotation, camera information, or keypoints </em>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present 3DMiner - a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets.
Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must exist images of objects with similar shapes but varying backgrounds, textures, and viewpoints.
Our approach leverages the recent advances in learning self-supervised image representations to cluster images with geometrically similar shapes and find common image correspondences between them.
We then exploit these correspondences to obtain rough camera estimates as initialization for bundle-adjustment.
Finally, for every image cluster, we apply a progressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape.
We show that this procedure is robust to several types of errors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild.
When using images from Pix3D chairs, our method is capable of producing significantly better results than state-of-the-art unsupervised 3D
reconstruction techniques, both quantitatively and qualitatively.
Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstructing shapes present in images from the LAION-5B dataset.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->

<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">3DMiner Pipeline </h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/pipeline.jpg" alt="3DMiner Overview"/>
           <div class="content has-text-justified">
             <p>
               Our method starts by grouping images that depict similar 3D shapes, regardless of the texture of the shape, the camera view-point or the background. To do so, we perturb each image  with various transformations (e.g. color jittering, perspective and rotation) and we pool their DINO-ViT features to create a robust image embedding.
We cluster images by running agglomerative clustering on the embeddings.
Within each cluster, we find key point correspondences using dense DINO-ViT features. We feed those corresponding keypoints to a Structure from Motion algorithm (rigid factorization) to get coarse orthographic camera estimations.
Finally, we jointly refine the camera parameters and learn an occupancy field to get the final shape.
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->

      <h2 class="title is-3">Reconstruction Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->

           <!-- <div style="width: 23%; float: left;"><img style="width: 50%" src="static/images/pickelhaube_ref.png" alt="3DMiner Overview"/> </div>
           <div style="width: 23%; float: left;"><img style="width: 50%" src="static/images/umbrella_ref.png" alt="3DMiner Overview"/> </div>
           <div style="width: 23%; float: left;"><img style="width: 50%" src="static/images/ps5_controller_ref.png" alt="3DMiner Overview"/> </div>
           <div style="width: 23%; float: left;"><img style="width: 50%" src="static/images/tobacco_pipe_ref.png" alt="3DMiner Overview"/> </div> -->


           <img style="width: 23%; float: left;" src="static/images/pickelhaube.gif" alt="3DMiner Overview"/>
           <img style="width: 23%; float: left;" src="static/images/umbrella.gif" alt="3DMiner Overview"/>
           <img style="width: 23%; float: left;" src="static/images/ps5_controller.gif" alt="3DMiner Overview"/>
           <img style="width: 23%; float: left;" src="static/images/tobacco_pipe.gif" alt="3DMiner Overview"/>

           <img src="static/images/gallery.jpg" alt="Results Gallery"/>
           <p>Qualitative Results on Pix3D and LAION-5B. To the best of our knowledge <em>we are the first to perform reconstruction on the challenging LAION-5B image dataset</em>.</p>
           <br><br>
           <img src="static/images/results.png" alt="Tabke" width="700" />
           <p>Quantitative Comparisons against SMR and Unicorn on Pix3D Chairs.</p>

           <br><br>
           <img src="static/images/threshold.jpg" alt="Reprojection Thresholding" width="500" />
           <p>We plot the reprojection error per cluster (averaged over each image in the cluster) in ascending order and show representative
             reconstructions for four data points. We empirically observe that the reprojection error is a good indicator of the quality of the reconstruction. </p>

      <!-- Paper video. -->


          <!-- </div> -->
        </div>
        <!-- <div class="column is-four-fifths">
          <img src="static/images/results.pdf" alt="3DMiner Overview"/>
        </div> -->
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Clustering Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/clusters.jpg" alt="Tabke" width="900" />
           <p>We show the clustering results on Pix3D Chairs when applying our augmentation scheme.</p>

           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>








<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{cheng2023Miner,
  title={3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets},
  author={Cheng, Ta-Ying and Gadelha, Matheus and Pirk, Soren and Groueix, Thibault and Mech, Radomir and Markham, Andrew and Trigoni, Niki},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
